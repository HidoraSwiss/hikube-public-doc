---
sidebar_position: 2
title: Architecture
---

# Architecture Hikube Kubernetes

Le schÃ©ma, ci-aprÃ¨s, illustre la structure et les interactions principales du **cluster Kubernetes Hikube**, incluant la haute disponibilitÃ© du plan de contrÃ´le, la gestion des nÅ“uds, la persistance des donnÃ©es, et la rÃ©plication inter-rÃ©gions.

<div class="only-light">
  <img src="/img/hikube-kubernetes-architecture.svg" alt="Logo clair"/>
</div>
<div class="only-dark">
  <img src="/img/hikube-kubernetes-architecture-dark.svg" alt="Logo sombre"/>
</div>
---

## ğŸ§© 1. Composants principaux du cluster

### ğŸ”¹ Etcd Cluster

- Contient plusieurs instances dâ€™**etcd** rÃ©pliquÃ©es entre elles.
- Assure la **cohÃ©rence du stockage dâ€™Ã©tat du cluster Kubernetes** (informations sur les pods, services, configurations, etc.).
- La rÃ©plication interne entre les nÅ“uds `etcd` garantit la **tolÃ©rance aux pannes**.

### ğŸ”¹ Control Plane

- ComposÃ© de lâ€™API Server, du Scheduler et du Controller Manager.
- RÃ´le :
  - **Planifie les workloads** (pods, dÃ©ploiements, etc.) sur les nÅ“uds disponibles.
  - **Interagit avec etcd** pour lire/Ã©crire lâ€™Ã©tat du cluster.

### ğŸ”¹ Node Groups

- Chaque groupe contient plusieurs **nÅ“uds de travail (worker nodes)**.
- Les workloads (pods) sont dÃ©ployÃ©s sur ces nÅ“uds.
- Les nÅ“uds communiquent avec le Control Plane pour recevoir leurs tÃ¢ches.
- Ils lisent et Ã©crivent leurs donnÃ©es dans les **Persistent Volume (PV)** Kubernetes.

### ğŸ”¹ Kubernetes PV Data

- ReprÃ©sente le **stockage persistant** utilisÃ© par les pods.
- Les donnÃ©es des workloads sont **Ã©crites et lues depuis ce stockage**.
- Cette couche est intÃ©grÃ©e Ã  la rÃ©plication Hikube pour garantir la disponibilitÃ© des donnÃ©es.

---

## ğŸ—„ï¸ 2. Couche de rÃ©plication Hikube

### Hikube Replication Data Layer

- Sert dâ€™interface entre Kubernetes et les **systÃ¨mes de stockage rÃ©gionaux**.
- RÃ©plique automatiquement les donnÃ©es des PV vers plusieurs rÃ©gions pour :
  - la **haute disponibilitÃ©**,
  - la **rÃ©silience aux pannes rÃ©gionales**,
  - et la **continuitÃ© de service**.

### Stockages rÃ©gionaux

- **Region 1** â†’ Geneva Data Storage
- **Region 2** â†’ Gland Data Storage
- **Region 3** â†’ Lucerne Data Storage

Chaque rÃ©gion dispose de son propre backend de stockage, tous synchronisÃ©s via la couche Hikube.

---

## ğŸ” 3. Flux de communication

1. Les **nÅ“uds etcd** se synchronisent entre eux pour maintenir un Ã©tat global cohÃ©rent.
2. Le **Control Plane** lit/Ã©crit dans etcd pour stocker lâ€™Ã©tat du cluster.
3. Le **Control Plane** planifie les workloads sur les **Node Groups**.
4. Les **Node Groups** interagissent avec les **PV Kubernetes** pour stocker ou rÃ©cupÃ©rer des donnÃ©es.
5. Les **PV Data** sont rÃ©pliquÃ©es Ã  travers la **Hikube Replication Data Layer** vers les **3 rÃ©gions**.

---

## âš™ï¸ 4. RÃ©sumÃ© fonctionnel

| Couche | Fonction principale | Technologie |
|--------|---------------------|-------------|
| Etcd Cluster | Stockage de lâ€™Ã©tat du cluster | etcd |
| Control Plane | Gestion et planification des workloads | Kubernetes |
| Node Groups | ExÃ©cution des workloads | kubelet, container runtime |
| PV Data | Stockage persistant | Kubernetes Persistent Volumes |
| Hikube Data Layer | RÃ©plication et synchronisation multi-rÃ©gions | Hikube |
| Data Storage | Stockage physique rÃ©gional | Geneva / Gland / Lucerne |

---

## ğŸŒ 5. Objectif global

Cette architecture assure :

- **Haute disponibilitÃ©** du cluster Kubernetes.
- **RÃ©silience gÃ©ographique** grÃ¢ce Ã  la rÃ©plication inter-rÃ©gions.
- **IntÃ©gritÃ© des donnÃ©es** via etcd et le stockage persistant.
- **ScalabilitÃ©** horizontale avec les Node Groups.

---
