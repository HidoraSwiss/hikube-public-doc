---
sidebar_position: 3
title: R√©f√©rence API - VM GPU
---

# API Reference - Machines Virtuelles GPU

Cette r√©f√©rence compl√®te d√©taille l'API **VirtualMachine GPU** d'Hikube, les configurations GPU, types d'instances optimis√©es et bonnes pratiques pour l'acc√©l√©ration mat√©rielle.

---

## üéÆ VirtualMachine GPU

### **Vue d'ensemble**

L'API `VirtualMachine` avec support GPU permet de cr√©er des machines virtuelles avec acc√®s direct aux acc√©l√©rateurs NVIDIA via GPU Passthrough.

```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
metadata:
  name: example-gpu-vm
spec:
  running: true
  instanceProfile: ubuntu
  instanceType: g1.xlarge
  gpus:
    - name: "nvidia.com/L40S"
  # Configuration d√©taill√©e ci-dessous
```

### **Sp√©cification GPU Compl√®te**

#### **Param√®tres GPU**

| **Param√®tre** | **Type** | **Description** | **D√©faut** | **Requis** |
|---------------|----------|-----------------|------------|------------|
| `gpus` | `[]GPU` | Liste des GPUs √† attacher √† la VM | `[]` | ‚úÖ |
| `gpus[].name` | `string` | Type de GPU NVIDIA √† allouer | - | ‚úÖ |
| `gpus[].deviceName` | `string` | Nom sp√©cifique du p√©riph√©rique GPU | auto | ‚ùå |

#### **Types de GPUs Disponibles**

##### **NVIDIA L40S (Ada Lovelace)**
```yaml
gpus:
  - name: "nvidia.com/L40S"
```
- **Architecture** : Ada Lovelace
- **M√©moire** : 48 GB GDDR6 avec ECC
- **Performance** : 362 TOPS (INT8), 91.6 TFLOPs (FP32)
- **Usage** : IA g√©n√©rative, rendu temps r√©el, simulation

##### **NVIDIA A100 (Ampere)**
```yaml
gpus:
  - name: "nvidia.com/A100"
```
- **Architecture** : Ampere
- **M√©moire** : 80 GB HBM2e avec ECC
- **Performance** : 312 TOPS (INT8), 624 TFLOPs (Tensor)
- **Usage** : Entra√Ænement IA, calcul haute performance

##### **NVIDIA H100 (Hopper)**
```yaml
gpus:
  - name: "nvidia.com/H100"
```
- **Architecture** : Hopper
- **M√©moire** : 80 GB HBM3 avec ECC
- **Performance** : 1979 TOPS (INT8), 989 TFLOPs (Tensor)
- **Usage** : LLM, transformers, calcul exascale

#### **Types d'Instances GPU**

##### **S√©rie G1 (GPU Single) - 1 GPU**
Optimis√©e pour workloads mono-GPU avec CPU et m√©moire √©quilibr√©s.

```yaml
# Instances G1 disponibles
instanceType: "g1.medium"    # 8 vCPU, 32 GB RAM, 1 GPU
instanceType: "g1.large"     # 16 vCPU, 64 GB RAM, 1 GPU
instanceType: "g1.xlarge"    # 32 vCPU, 128 GB RAM, 1 GPU
instanceType: "g1.2xlarge"   # 64 vCPU, 256 GB RAM, 1 GPU
instanceType: "g1.4xlarge"   # 128 vCPU, 512 GB RAM, 1 GPU
```

##### **S√©rie G2 (GPU Dual) - 2 GPUs**
Optimis√©e pour workloads multi-GPU avec parall√©lisation.

```yaml
# Instances G2 disponibles
instanceType: "g2.large"     # 32 vCPU, 128 GB RAM, 2 GPUs
instanceType: "g2.xlarge"    # 64 vCPU, 256 GB RAM, 2 GPUs
instanceType: "g2.2xlarge"   # 128 vCPU, 512 GB RAM, 2 GPUs
instanceType: "g2.4xlarge"   # 256 vCPU, 1 TB RAM, 2 GPUs
```

##### **S√©rie G4 (GPU Quad) - 4 GPUs**
Optimis√©e pour calcul intensif et entra√Ænement de mod√®les complexes.

```yaml
# Instances G4 disponibles
instanceType: "g4.xlarge"    # 64 vCPU, 256 GB RAM, 4 GPUs
instanceType: "g4.2xlarge"   # 128 vCPU, 512 GB RAM, 4 GPUs
instanceType: "g4.4xlarge"   # 256 vCPU, 1 TB RAM, 4 GPUs
instanceType: "g4.8xlarge"   # 512 vCPU, 2 TB RAM, 4 GPUs
```

##### **S√©rie G8 (GPU Octo) - 8 GPUs**
Optimis√©e pour l'entra√Ænement de LLM et calcul distribu√©.

```yaml
# Instances G8 disponibles
instanceType: "g8.2xlarge"   # 128 vCPU, 512 GB RAM, 8 GPUs
instanceType: "g8.4xlarge"   # 256 vCPU, 1 TB RAM, 8 GPUs
instanceType: "g8.8xlarge"   # 512 vCPU, 2 TB RAM, 8 GPUs
```

### **Configuration Multi-GPU**

#### **Configuration Dual GPU**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
metadata:
  name: dual-gpu-vm
spec:
  running: true
  instanceProfile: ubuntu
  instanceType: g2.xlarge
  gpus:
    - name: "nvidia.com/A100"
    - name: "nvidia.com/A100"
  systemDisk:
    size: 200Gi
    storageClass: replicated
  cloudInit: |
    #cloud-config
    runcmd:
      - nvidia-smi
      - nvidia-smi topo -m  # V√©rifier la topologie GPU
```

#### **Configuration Quad GPU pour HPC**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
metadata:
  name: hpc-quad-gpu
spec:
  running: true
  instanceProfile: ubuntu
  instanceType: g4.2xlarge
  gpus:
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
  systemDisk:
    size: 500Gi
    storageClass: replicated
  cloudInit: |
    #cloud-config
    packages:
      - nvidia-cuda-toolkit
      - openmpi-bin
      - openmpi-common
    runcmd:
      - nvidia-smi
      - mpirun --version
```

### **Stockage Optimis√© GPU**

#### **Storage Classes Recommand√©es**

```yaml
# Pour datasets IA/ML
systemDisk:
  size: 1000Gi
  storageClass: "replicated"

# Stockage temporaire haute performance
additionalDisks:
  - name: scratch-disk
    size: 2000Gi
    storageClass: "local"  # NVMe local ultra-rapide
```

#### **Configuration Stockage par Use Case**

##### **Entra√Ænement IA**
```yaml
systemDisk:
  size: 200Gi
  storageClass: "replicated"
additionalDisks:
  - name: datasets
    size: 5000Gi
    storageClass: "replicated"
  - name: checkpoints
    size: 1000Gi
    storageClass: "replicated"
  - name: temp-data
    size: 2000Gi
    storageClass: "local"
```

##### **Inf√©rence Production**
```yaml
systemDisk:
  size: 100Gi
  storageClass: "replicated"
additionalDisks:
  - name: models
    size: 500Gi
    storageClass: "replicated"
```

##### **Rendu 3D**
```yaml
systemDisk:
  size: 200Gi
  storageClass: "replicated"
additionalDisks:
  - name: scenes
    size: 2000Gi
    storageClass: "replicated"
  - name: cache
    size: 1000Gi
    storageClass: "local"
```

### **R√©seau Haute Performance**

#### **Configuration pour Multi-GPU**
```yaml
spec:
  network:
    interfaces:
      - name: default
        bridge: {}
      - name: hpc
        multus:
          networkName: "gpu-interconnect"  # R√©seau InfiniBand/RoCE
```

### **Cloud-Init pour GPU**

#### **Configuration de base avec Drivers NVIDIA**
```yaml
cloudInit: |
  #cloud-config
  users:
    - name: gpu-user
      sudo: ALL=(ALL) NOPASSWD:ALL
      shell: /bin/bash
      groups: [docker]
      ssh_authorized_keys:
        - ssh-rsa AAAAB3NzaC1yc2E...

  package_update: true
  packages:
    - curl
    - wget
    - build-essential
    - dkms

  runcmd:
    # Installation drivers NVIDIA
    - wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
    - dpkg -i cuda-keyring_1.0-1_all.deb
    - apt-get update
    - apt-get install -y cuda-toolkit nvidia-driver-535
    
    # Configuration persistance
    - systemctl enable nvidia-persistenced
    - nvidia-smi -pm 1
    
    # Installation Docker avec support NVIDIA
    - curl -fsSL https://get.docker.com | sh
    - distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    - curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
    - curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    - apt-get update
    - apt-get install -y nvidia-container-toolkit
    - nvidia-ctk runtime configure --runtime=docker
    - systemctl restart docker
```

#### **Configuration pour Deep Learning**
```yaml
cloudInit: |
  #cloud-config
  runcmd:
    # Drivers et CUDA comme ci-dessus
    - # ... configuration drivers ...
    
    # Installation PyTorch
    - pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    
    # Installation TensorFlow
    - pip3 install tensorflow[and-cuda]
    
    # Installation Jupyter Lab
    - pip3 install jupyterlab
    - jupyter lab --generate-config
    
    # Optimisations GPU
    - echo 'export CUDA_CACHE_PATH=/tmp' >> /etc/environment
    - echo 'export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128' >> /etc/environment
```

### **Monitoring et Observabilit√©**

#### **M√©triques GPU**
```yaml
# Annotations pour le monitoring
metadata:
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9400"
    prometheus.io/path: "/metrics"

cloudInit: |
  #cloud-config
  runcmd:
    # Installation DCGM Exporter pour Prometheus
    - docker run -d --gpus all --rm -p 9400:9400 nvcr.io/nvidia/k8s/dcgm-exporter:3.1.8-3.1.5-ubuntu20.04
```

### **Exemples d'Usage**

#### **VM pour Entra√Ænement LLM**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
metadata:
  name: llm-training
spec:
  running: true
  instanceProfile: ubuntu
  instanceType: g8.4xlarge  # 8x H100
  gpus:
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
    - name: "nvidia.com/H100"
  systemDisk:
    size: 500Gi
    storageClass: replicated
  additionalDisks:
    - name: datasets
      size: 10000Gi  # 10TB pour datasets
      storageClass: replicated
    - name: checkpoints
      size: 5000Gi   # 5TB pour checkpoints
      storageClass: replicated
```

#### **VM pour Inf√©rence API**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
metadata:
  name: inference-api
spec:
  running: true
  instanceProfile: ubuntu
  instanceType: g1.large
  gpus:
    - name: "nvidia.com/L40S"
  external: true
  externalPorts:
    - 8000  # API endpoint
  systemDisk:
    size: 200Gi
    storageClass: replicated
  cloudInit: |
    #cloud-config
    runcmd:
      # Installation TensorRT pour optimisation
      - apt-get install -y tensorrt
      # Configuration FastAPI
      - pip3 install fastapi uvicorn transformers
```

#### **VM pour Rendu 3D**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
metadata:
  name: render-farm
spec:
  running: true
  instanceProfile: ubuntu
  instanceType: g2.xlarge
  gpus:
    - name: "nvidia.com/L40S"
    - name: "nvidia.com/L40S"
  systemDisk:
    size: 500Gi
    storageClass: replicated
  external: true
  externalPorts:
    - 5900  # VNC pour acc√®s graphique
  cloudInit: |
    #cloud-config
    packages:
      - ubuntu-desktop-minimal
      - blender
      - x11vnc
    runcmd:
      - systemctl set-default graphical.target
```

### **Bonnes Pratiques**

#### **üîß Optimisation Performance**

1. **CPU/GPU Ratio** : Utilisez 8-16 vCPU par GPU pour √©quilibrer les workloads
2. **M√©moire** : Minimum 32GB RAM par GPU, 64-128GB pour entra√Ænement
3. **Stockage** : Utilisez du stockage local pour datasets temporaires
4. **R√©seau** : Configurez RDMA pour multi-GPU distribu√©

#### **üí∞ Optimisation Co√ªts**

1. **Auto-scaling** : Arr√™tez les VMs quand inutilis√©es
2. **Spot instances** : Utilisez pour workloads interruptibles
3. **Scheduling** : Planifiez les entra√Ænements pendant heures creuses
4. **Monitoring** : Surveillez l'utilisation GPU pour dimensionner

#### **üîí S√©curit√©**

1. **Isolation** : Un GPU = Une VM, pas de partage
2. **Chiffrement** : Activez le chiffrement des donn√©es sensibles
3. **Acc√®s** : Limitez l'acc√®s r√©seau aux ports n√©cessaires
4. **Audit** : Loggez tous les acc√®s GPU et transferts de donn√©es

:::tip Performance GPU üöÄ
Pour maximiser les performances, utilisez des images optimis√©es avec drivers pr√©-install√©s et configurez CUDA Unified Memory pour les gros datasets.
:::

:::warning Limitation H100 ‚ö†Ô∏è
Les instances H100 sont limit√©es en nombre. R√©servez √† l'avance pour les projets critiques et lib√©rez rapidement apr√®s usage.
:::

---

**Besoin d'aide ?** Consultez notre [guide de troubleshooting GPU](./troubleshooting.md) ou contactez le support Hikube ! üÜò 