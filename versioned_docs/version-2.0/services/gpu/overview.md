---
sidebar_position: 1
title: Vue d'ensemble des GPU
---

# GPUs sur Hikube

Hikube propose l'acc√®s aux acc√©l√©rateurs **NVIDIA** via GPU Passthrough, permettant l'ex√©cution de workloads n√©cessitant une acc√©l√©ration mat√©rielle. Les GPU sont disponibles pour deux types de workloads : machines virtuelles et pods Kubernetes.

---

## üéØ Types d'Utilisation

### **GPU avec Machines Virtuelles**
Les GPU peuvent √™tre attach√©s directement aux machines virtuelles via GPU passthrough VFIO-PCI, offrant un acc√®s complet et exclusif √† l'acc√©l√©rateur.

**Cas d'usage :**
- Applications n√©cessitant un contr√¥le complet du GPU
- Workloads legacy ou sp√©cialis√©s
- Environnements de d√©veloppement isol√©s
- Applications graphiques (rendu, CAO)

### **GPU avec Kubernetes**
Les GPU peuvent √™tre allou√©s aux workers Kubernetes et ensuite assign√©s aux pods via les resource requests/limits.

**Cas d'usage :**
- Workloads containeris√©s d'IA/ML
- Scaling automatique des applications GPU
- Partage de ressources GPU entre applications
- Orchestration complexe de jobs parall√®les

---

## üñ•Ô∏è Hardware Disponible

Hikube propose trois types de GPU NVIDIA :

### **NVIDIA L40S**
- **Architecture** : Ada Lovelace
- **M√©moire** : 48 GB GDDR6 avec ECC
- **Performance** : 362 TOPS (INT8), 91.6 TFLOPs (FP32)
- **Usage typique** : IA g√©n√©rative, inf√©rence, rendu temps r√©el

### **NVIDIA A100**
- **Architecture** : Ampere
- **M√©moire** : 80 GB HBM2e avec ECC
- **Performance** : 312 TOPS (INT8), 624 TFLOPs (Tensor)
- **Usage typique** : Entra√Ænement ML, calcul haute performance

### **NVIDIA H100**
- **Architecture** : Hopper
- **M√©moire** : 80 GB HBM3 avec ECC
- **Performance** : 1979 TOPS (INT8), 989 TFLOPs (Tensor)
- **Usage typique** : LLM, transformers, calcul exascale

---

## üèóÔ∏è Architecture

### **Allocation GPU avec VMs**

```mermaid
flowchart TD
    subgraph HIKUBE["Infrastructure Hikube"]
        subgraph NODE["N≈ìud Physique"]
            GPU1["üéÆ GPU L40S"]
            GPU2["üéÆ GPU A100"]
            GPU3["üéÆ GPU H100"]
        end
        
        subgraph VM1["VM Instance"]
            APP1["Application GPU"]
        end
    end
    
    GPU1 --> VM1
    VM1 --> APP1
    
    style GPU1 fill:#90EE90
    style VM1 fill:#FFE4B5
    style APP1 fill:#ADD8E6
```

### **Allocation GPU avec Kubernetes**

```mermaid
flowchart TD
    subgraph CLUSTER["Cluster Kubernetes"]
        subgraph WORKER["Worker Node"]
            GPU1["üéÆ GPU L40S"]
            GPU2["üéÆ GPU A100"]
            KUBELET["kubelet"]
        end
        
        subgraph POD1["Pod"]
            CONTAINER1["Container GPU"]
        end
        
        subgraph POD2["Pod"]
            CONTAINER2["Container GPU"]
        end
    end
    
    GPU1 --> KUBELET
    GPU2 --> KUBELET
    KUBELET --> POD1
    KUBELET --> POD2
    POD1 --> CONTAINER1
    POD2 --> CONTAINER2
    
    style GPU1 fill:#90EE90
    style GPU2 fill:#90EE90
    style POD1 fill:#FFE4B5
    style POD2 fill:#FFE4B5
```

---

## ‚öôÔ∏è Configuration

### **GPU sur VM**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: VirtualMachine
spec:
  instanceType: "u1.xlarge"
  gpus:
    - name: "nvidia.com/AD102GL_L40S"
```

### **GPU sur Kubernetes Worker**
```yaml
apiVersion: apps.cozystack.io/v1alpha1
kind: Kubernetes
spec:
  nodeGroups:
    gpu-workers:
      instanceType: "u1.xlarge"
      gpus:
        - name: "nvidia.com/AD102GL_L40S"
```

### **GPU dans Pod Kubernetes**
```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: gpu-app
    image: nvidia/cuda:12.0-runtime-ubuntu20.04
    resources:
      limits:
        nvidia.com/gpu: 1
```

---

## üìã Comparaison des Approches

| **Aspect** | **GPU sur VM** | **GPU sur Kubernetes** |
|------------|----------------|------------------------|
| **Isolation** | Compl√®te (1 GPU = 1 VM) | Partag√©e (orchestr√©e) |
| **Performance** | Native (passthrough) | Native (device plugin) |
| **Gestion** | Manuelle | Automatis√©e |
| **Scaling** | Vertical uniquement | Horizontal + Vertical |
| **Partage** | Non | Oui (entre pods) |
| **Complexit√©** | Simple | Complexe |

---

## üöÄ Prochaines √âtapes

### **Pour les Machines Virtuelles**
- [Cr√©er une VM GPU](./quick-start.md) ‚Üí Guide pratique
- [API Reference](./api-reference.md) ‚Üí Configuration compl√®te

### **Pour Kubernetes**
- [Clusters GPU](../kubernetes/overview.md) ‚Üí Workers avec GPU
  - [Configuration avanc√©e](../kubernetes/api-reference.md) ‚Üí NodeGroups GPU
 