---
title: Kafka sur Hikube
---

# Kafka - Plateforme de streaming de donn√©es

**Kafka** sur Hikube offre une plateforme de streaming de donn√©es distribu√©e et haute performance. Ce service vous permet de traiter des flux de donn√©es en temps r√©el avec une fiabilit√© et une scalabilit√© exceptionnelles.

---

## Qu'est-ce que Kafka ?

Apache Kafka est une plateforme de streaming distribu√©e qui permet de publier, stocker et traiter des flux de donn√©es en temps r√©el. Sur Hikube, Kafka est d√©ploy√© avec haute disponibilit√© et gestion automatique des partitions.

### Avantages sur Hikube

- **üöÄ Performance** : D√©bit ultra-√©lev√© (millions de messages/seconde)
- **üîí Fiabilit√©** : R√©plication automatique des donn√©es
- **üìà Scalabilit√©** : Scaling horizontal automatique
- **üíæ Persistance** : Stockage persistant des messages
- **üîß Simplicit√©** : Configuration d√©clarative
- **üìä Monitoring** : M√©triques int√©gr√©es

### Cas d'usage

- **Event Streaming** : Flux d'√©v√©nements en temps r√©el
- **Data Pipeline** : Pipelines de donn√©es ETL
- **Microservices** : Communication inter-services
- **IoT** : Donn√©es de capteurs et t√©l√©matique
- **Logs** : Centralisation des logs applicatifs
- **Analytics** : Donn√©es pour l'analyse en temps r√©el

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Application Layer           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    Producer ‚îÇ Consumer ‚îÇ Stream     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Kafka Cluster               ‚îÇ
‚îÇ  Broker 1 ‚îÇ Broker 2 ‚îÇ Broker 3    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Storage Layer               ‚îÇ
‚îÇ    (Distributed Storage)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants

- **Kafka Brokers** : Serveurs qui stockent les messages
- **ZooKeeper** : Coordination et m√©tadonn√©es
- **Producers** : Applications qui publient des messages
- **Consumers** : Applications qui consomment des messages
- **Topics** : Canaux de messages organis√©s
- **Partitions** : Unit√©s de parall√©lisme

---

## Fonctionnalit√©s Kafka

### Concepts fondamentaux

- **Topics** : Canaux de messages nomm√©s
- **Partitions** : Unit√©s de parall√©lisme dans un topic
- **Replicas** : Copies des partitions pour la fiabilit√©
- **Producers** : Applications qui publient des messages
- **Consumers** : Applications qui lisent des messages
- **Consumer Groups** : Groupes de consumers qui partagent la charge

### Fonctionnalit√©s avanc√©es

- **Exactly Once Semantics** : Garantie de traitement unique
- **Stream Processing** : Traitement de flux avec KSQL
- **Schema Registry** : Gestion des sch√©mas de donn√©es
- **Connect** : Connecteurs pour syst√®mes externes
- **Security** : Authentification et autorisation
- **Monitoring** : M√©triques d√©taill√©es

---

## Comparaison avec d'autres solutions

| Fonctionnalit√© | Kafka Hikube | AWS MSK | GCP Pub/Sub | Azure Event Hubs |
|----------------|---------------|---------|-------------|------------------|
| **Performance** | ‚ö° Ultra-√©lev√©e | ‚ö° √âlev√©e | ‚ö° √âlev√©e | ‚ö° √âlev√©e |
| **Setup** | ‚ö° Instantan√© | ‚ö° Instantan√© | ‚ö° Instantan√© | ‚ö° Instantan√© |
| **Co√ªt** | üí∞ Pr√©visible | üí∞ Variable | üí∞ Variable | üí∞ Variable |
| **K8s Integration** | ‚úÖ Native | ‚ö†Ô∏è Partielle | ‚ö†Ô∏è Partielle | ‚ö†Ô∏è Partielle |
| **Stream Processing** | ‚úÖ KSQL | ‚úÖ KSQL | ‚ö†Ô∏è Limit√© | ‚ö†Ô∏è Limit√© |
| **Schema Registry** | ‚úÖ Int√©gr√© | ‚úÖ Int√©gr√© | ‚ö†Ô∏è Partiel | ‚ö†Ô∏è Partiel |

---

## Int√©gration avec l'√©cosyst√®me Hikube

### Kubernetes

Kafka s'int√®gre parfaitement avec Kubernetes :

- **Custom Resources** : D√©finition d√©clarative
- **Operators** : Gestion automatique
- **Services** : D√©couverte de service automatique
- **Secrets** : Gestion s√©curis√©e des credentials
- **ConfigMaps** : Configuration centralis√©e

### Applications

Int√©gration avec les autres services Hikube :

- **ClickHouse** : Ingestion de donn√©es analytiques
- **PostgreSQL** : CDC (Change Data Capture)
- **Redis** : Cache de messages
- **Monitoring** : M√©triques et alertes

---

## Exemples d'usage

### Producer simple

```text
from kafka import KafkaProducer
import json

# Configuration du producer
producer = KafkaProducer(
    bootstrap_servers=['kafka-mon-app.default.svc.cluster.local:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    key_serializer=lambda k: k.encode('utf-8') if k else None
)

# Publier des messages
# Exemple de fonction send_event(event_type, data):
message = {
    'event_type': 'user_login',
    'data': {'user_id': 123, 'ip': '192.168.1.1'},
    'timestamp': datetime.now().isoformat()
}

producer.send('events', key='user_login', value=message)
producer.flush()

# Exemples d'utilisation
# send_event('user_login', {'user_id': 123, 'ip': '192.168.1.1'})
# send_event('purchase', {'user_id': 123, 'amount': 99.99, 'product': 'premium'})
# send_event('page_view', {'user_id': 456, 'page': '/home', 'session_id': 'sess_1'})
```

### Consumer simple

```text
from kafka import KafkaConsumer
import json

# Configuration du consumer
consumer = KafkaConsumer(
    'events',
    bootstrap_servers=['kafka-mon-app.default.svc.cluster.local:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    key_deserializer=lambda k: k.decode('utf-8') if k else None,
    group_id='event-processor',
    auto_offset_reset='earliest'
)

# Traiter les messages
for message in consumer:
    event = message.value
    event_type = message.key
    
    print(f"Received {event_type} event: {event}")
    
    # Traitement selon le type d'√©v√©nement
    if event_type == 'user_login':
        # process_login(event['data'])
        print("Processing login event")
    elif event_type == 'purchase':
        # process_purchase(event['data'])
        print("Processing purchase event")
    elif event_type == 'page_view':
        # process_page_view(event['data'])
        print("Processing page view event")
```

### Stream Processing avec KSQL

```sql
-- Cr√©er un stream depuis un topic
CREATE STREAM user_events (
    user_id BIGINT,
    event_type VARCHAR,
    data VARCHAR,
    timestamp VARCHAR
) WITH (
    kafka_topic='events',
    value_format='JSON'
);

-- Filtrer les √©v√©nements de connexion
CREATE STREAM login_events AS
SELECT user_id, data, timestamp
FROM user_events
WHERE event_type = 'user_login';

-- Agr√©gation par utilisateur
CREATE TABLE user_activity AS
SELECT 
    user_id,
    COUNT(*) as event_count,
    COLLECT_LIST(event_type) as event_types,
    MAX(timestamp) as last_activity
FROM user_events
GROUP BY user_id;

-- D√©tection d'activit√© suspecte
CREATE STREAM suspicious_activity AS
SELECT 
    user_id,
    COUNT(*) as login_count,
    WINDOWEND as window_end
FROM login_events
WINDOW TUMBLING (SIZE 1 HOUR)
GROUP BY user_id
HAVING COUNT(*) > 10;
```

### Schema Registry

```text
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer, AvroConsumer
from confluent_kafka.avro.serializer import SerializerError

# Configuration avec Schema Registry
producer_config = {
    'bootstrap.servers': 'kafka-mon-app.default.svc.cluster.local:9092',
    'schema.registry.url': 'http://kafka-mon-app.default.svc.cluster.local:8081'
}

# Sch√©ma Avro pour les √©v√©nements
event_schema = {
    "type": "record",
    "name": "Event",
    "fields": [
        {"name": "user_id", "type": "long"},
        {"name": "event_type", "type": "string"},
        {"name": "data", "type": "string"},
        {"name": "timestamp", "type": "string"}
    ]
}

# Producer avec sch√©ma
producer = AvroProducer(producer_config, default_value_schema=event_schema)

# Publier un message avec sch√©ma
event = {
    'user_id': 123,
    'event_type': 'purchase',
    'data': '{"amount": 99.99, "product": "premium"}',
    'timestamp': '2024-01-15T10:30:00Z'
}

producer.produce(topic='events', value=event)
producer.flush()
```

### Connect pour PostgreSQL

```yaml
# Configuration du connecteur PostgreSQL
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: postgres-source
  labels:
    strimzi.io/cluster: kafka-mon-app
spec:
  class: io.confluent.connect.jdbc.JdbcSourceConnector
  tasksMax: 1
  config:
    connection.url: jdbc:postgresql://postgres-mon-app.default.svc.cluster.local:5432/monapp
    connection.user: admin
    connection.password: mon-mot-de-passe-securise
    topic.prefix: postgres-
    mode: incrementing
    incrementing.column.name: id
    table.whitelist: users,orders,products
    poll.interval.ms: 5000
```

### Consumer Group avec r√©partition

```text
from kafka import KafkaConsumer
import json
import threading

# Exemple de classe EventProcessor:
# class EventProcessor:
#     def __init__(self, consumer_id):
#         self.consumer_id = consumer_id
#         self.consumer = KafkaConsumer(
#             'events',
#             bootstrap_servers=['kafka-mon-app.default.svc.cluster.local:9092'],
#             value_deserializer=lambda m: json.loads(m.decode('utf-8')),
#             group_id='event-processors',
#             auto_offset_reset='earliest'
#         )
#     
#     def process_messages(self):
#         print(f"Consumer {self.consumer_id} started")
#         
#         for message in self.consumer:
#             event = message.value
#             partition = message.partition
#             offset = message.offset
#             
#             print(f"Consumer {self.consumer_id} - Partition {partition} - Offset {offset}: {event}")
#             
#             # Traitement du message
#             self.process_event(event)
#     
#     def process_event(self, event):
#         # Logique de traitement sp√©cifique
#         if event['event_type'] == 'purchase':
#             self.process_purchase(event)
#         elif event['event_type'] == 'user_login':
#             self.process_login(event)
#     
#     def process_purchase(self, event):
#         # Traitement des achats
#         print(f"Processing purchase: {event['data']}")
#     
#     def process_login(self, event):
#         # Traitement des connexions
#         print(f"Processing login: {event['data']}")

# D√©marrer plusieurs consumers
consumers = []
for i in range(3):
    consumer = EventProcessor(f"consumer-{i}")
    thread = threading.Thread(target=consumer.process_messages)
    thread.start()
    consumers.append(consumer)
```

### Monitoring et m√©triques

```text
from kafka.admin import KafkaAdminClient, ConfigResource, ConfigResourceType
from kafka import KafkaConsumer
import json

# Client admin pour la gestion
admin_client = KafkaAdminClient(
    bootstrap_servers=['kafka-mon-app.default.svc.cluster.local:9092']
)

# Exemple d'obtention des informations sur les topics:
# def get_topic_info():
#     topics = admin_client.list_topics()
#     for topic in topics:
#         if not topic.startswith('__'):
#             print(f"Topic: {topic}")
#             
#             # Obtenir les partitions
#             partitions = admin_client.get_partition_metadata(topic)
#             print(f"  Partitions: {len(partitions)}")
#             
#             # Obtenir la configuration
#             config = admin_client.describe_configs([
#                 ConfigResource(ConfigResourceType.TOPIC, topic)
#             ])
#             print(f"  Config: {config}")

# Exemple de consumer pour les m√©triques:
# def monitor_consumer_lag():
#     consumer = KafkaConsumer(
#         group_id='monitoring',
#         bootstrap_servers=['kafka-mon-app.default.svc.cluster.local:9092'],
#         enable_auto_commit=False
#     )
#     
#     # Obtenir les m√©triques de lag
#     for topic in consumer.topics():
#         partitions = consumer.partitions_for_topic(topic)
#         for partition in partitions:
#             tp = TopicPartition(topic, partition)
#             beginning = consumer.beginning_offsets([tp])[tp]
#             end = consumer.end_offsets([tp])[tp]
#             
#             consumer.assign([tp])
#             current = consumer.position([tp])[tp]
#             
#             lag = end - current
#             print(f"Topic: {topic}, Partition: {partition}, Lag: {lag}")

# Exemple de monitoring des performances:
# def monitor_producer_performance():
#     from kafka import KafkaProducer
#     import time
#     
#     producer = KafkaProducer(
#         bootstrap_servers=['kafka-mon-app.default.svc.cluster.local:9092'],
#         value_serializer=lambda v: json.dumps(v).encode('utf-8')
#     )
#     
#     start_time = time.time()
#     messages_sent = 0
#     
#     for i in range(10000):
#         producer.send('test-topic', value={'message_id': i, 'data': f'Message {i}'})
#         messages_sent += 1
#         
#         if i % 1000 == 0:
#             elapsed = time.time() - start_time
#             rate = messages_sent / elapsed
#             print(f"Messages sent: {messages_sent}, Rate: {rate:.2f} msg/s")
#     
#     producer.flush()
#     total_time = time.time() - start_time
#     print(f"Total time: {total_time:.2f}s, Average rate: {messages_sent/total_time:.2f} msg/s")
```

---

## Prochaines √©tapes

1. **[D√©marrage rapide](quick-start.md)** : D√©ployez Kafka en 5 minutes
2. **[R√©f√©rence API](api-reference.md)** : Tous les param√®tres disponibles
3. **[Tutoriels](tutorials/)** : Guides avanc√©s
4. **[D√©pannage](troubleshooting.md)** : Solutions aux probl√®mes courants
5. **[Connect](connect/)** : Connecteurs pour syst√®mes externes
6. **[KSQL](ksql/)** : Stream processing avec KSQL 