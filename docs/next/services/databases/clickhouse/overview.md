---
title: ClickHouse sur Hikube
---

# ClickHouse - Base de donn√©es analytique

**ClickHouse** sur Hikube offre une base de donn√©es analytique haute performance pour l'analyse de donn√©es en temps r√©el. Ce service vous permet d'analyser de grandes quantit√©s de donn√©es avec des requ√™tes SQL ultra-rapides.

---

## Qu'est-ce que ClickHouse ?

ClickHouse est une base de donn√©es analytique open-source d√©velopp√©e par Yandex, optimis√©e pour les requ√™tes OLAP (Online Analytical Processing). Elle excelle dans l'analyse de grandes quantit√©s de donn√©es avec des performances exceptionnelles.

### Avantages sur Hikube

- **üöÄ Performance** : Requ√™tes analytiques ultra-rapides
- **üìä Analytics** : Optimis√© pour l'analyse de donn√©es
- **üìà Scalabilit√©** : Cluster distribu√© automatique
- **üíæ Compression** : Stockage tr√®s efficace
- **üîß Simplicit√©** : Configuration d√©clarative
- **üìä Monitoring** : M√©triques int√©gr√©es

### Cas d'usage

- **Analytics** : Analyse de donn√©es en temps r√©el
- **Logs** : Analyse de logs applicatifs
- **IoT** : Donn√©es de capteurs et t√©l√©matique
- **E-commerce** : Analytics de ventes
- **Monitoring** : M√©triques syst√®me et applicatives
- **Business Intelligence** : Tableaux de bord

---

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Application Layer           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         ClickHouse Client           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Coordinator ‚îÇ Shard 1 ‚îÇ Shard 2    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Storage Layer               ‚îÇ
‚îÇ    (Distributed Storage)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants

- **ClickHouse Coordinator** : Coordination des requ√™tes
- **ClickHouse Shards** : N≈ìuds de donn√©es distribu√©s
- **ZooKeeper** : Coordination et m√©tadonn√©es
- **Storage** : Stockage distribu√© haute performance
- **Client** : Connexions depuis les applications

---

## Fonctionnalit√©s ClickHouse

### Moteurs de stockage

- **MergeTree** : Moteur principal pour donn√©es ordonn√©es
- **ReplacingMergeTree** : D√©duplication automatique
- **SummingMergeTree** : Agr√©gation automatique
- **AggregatingMergeTree** : Agr√©gations pr√©-calcul√©es
- **CollapsingMergeTree** : Gestion des changements
- **VersionedCollapsingMergeTree** : Versions des changements

### Fonctionnalit√©s avanc√©es

- **Columnar Storage** : Stockage orient√© colonnes
- **Vectorized Processing** : Traitement vectoris√©
- **Compression** : Compression automatique des donn√©es
- **Materialized Views** : Vues mat√©rialis√©es
- **Window Functions** : Fonctions de fen√™tre
- **User-Defined Functions** : Fonctions personnalis√©es

---

## Comparaison avec d'autres solutions

| Fonctionnalit√© | ClickHouse Hikube | AWS Redshift | GCP BigQuery | Azure Synapse |
|----------------|-------------------|--------------|--------------|---------------|
| **Performance** | ‚ö° Ultra-rapide | ‚ö° Rapide | ‚ö° Rapide | ‚ö° Rapide |
| **Setup** | ‚ö° Instantan√© | ‚ö° Instantan√© | ‚ö° Instantan√© | ‚ö° Instantan√© |
| **Co√ªt** | üí∞ Pr√©visible | üí∞ Variable | üí∞ Variable | üí∞ Variable |
| **K8s Integration** | ‚úÖ Native | ‚ö†Ô∏è Partielle | ‚ö†Ô∏è Partielle | ‚ö†Ô∏è Partielle |
| **Compression** | üì¶ Excellente | üì¶ Bonne | üì¶ Bonne | üì¶ Bonne |
| **Real-time** | ‚ö° Excellent | ‚ö†Ô∏è Limit√© | ‚ö†Ô∏è Limit√© | ‚ö†Ô∏è Limit√© |

---

## Int√©gration avec l'√©cosyst√®me Hikube

### Kubernetes

ClickHouse s'int√®gre parfaitement avec Kubernetes :

- **Custom Resources** : D√©finition d√©clarative
- **Operators** : Gestion automatique
- **Services** : D√©couverte de service automatique
- **Secrets** : Gestion s√©curis√©e des credentials
- **ConfigMaps** : Configuration centralis√©e

### Applications

Int√©gration avec les autres services Hikube :

- **Kafka** : Ingestion de donn√©es en streaming
- **PostgreSQL** : Donn√©es de r√©f√©rence
- **Redis** : Cache de requ√™tes
- **Monitoring** : M√©triques et alertes

---

## Exemples d'usage

### Connexion simple

```python
import clickhouse_driver

# Connexion √† ClickHouse
client = clickhouse_driver.Client(
    host='clickhouse-mon-app.default.svc.cluster.local',
    port=9000,
    user='default',
    password='mon-mot-de-passe-securise',
    database='analytics'
)

# Ex√©cuter une requ√™te
result = client.execute('SELECT * FROM events LIMIT 10')
for row in result:
    print(f"Event: {row}")
```

### Cr√©ation de tables

```sql
-- Table pour les √©v√©nements utilisateur
CREATE TABLE events (
    event_id UUID,
    user_id UInt32,
    event_type String,
    event_data String,
    timestamp DateTime,
    session_id String
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (user_id, timestamp)
TTL timestamp + INTERVAL 1 YEAR;

-- Table pour les m√©triques
CREATE TABLE metrics (
    metric_name String,
    metric_value Float64,
    labels Map(String, String),
    timestamp DateTime
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (metric_name, timestamp)
TTL timestamp + INTERVAL 6 MONTH;
```

### Insertion de donn√©es

```python
import clickhouse_driver
from datetime import datetime
import uuid

client = clickhouse_driver.Client(
    host='clickhouse-mon-app.default.svc.cluster.local',
    port=9000,
    user='default',
    password='mon-mot-de-passe-securise',
    database='analytics'
)

# Ins√©rer des √©v√©nements
events = [
    (str(uuid.uuid4()), 123, 'page_view', '{"page": "/home"}', datetime.now(), 'session_1'),
    (str(uuid.uuid4()), 123, 'click', '{"button": "signup"}', datetime.now(), 'session_1'),
    (str(uuid.uuid4()), 456, 'purchase', '{"amount": 99.99}', datetime.now(), 'session_2')
]

# Ex√©cuter l'insertion
client.execute(
    'INSERT INTO events (event_id, user_id, event_type, event_data, timestamp, session_id) VALUES',
    events
)

# Ins√©rer des m√©triques
metrics = [
    ('cpu_usage', 75.5, {'host': 'server1', 'region': 'eu'}, datetime.now()),
    ('memory_usage', 82.3, {'host': 'server1', 'region': 'eu'}, datetime.now()),
    ('disk_usage', 45.7, {'host': 'server2', 'region': 'us'}, datetime.now())
]

# Ex√©cuter l'insertion
client.execute(
    'INSERT INTO metrics (metric_name, metric_value, labels, timestamp) VALUES',
    metrics
)
```

### Requ√™tes analytiques

```sql
-- Analyse des √©v√©nements par utilisateur
SELECT 
    user_id,
    count() as event_count,
    uniq(session_id) as session_count,
    min(timestamp) as first_event,
    max(timestamp) as last_event
FROM events
WHERE timestamp >= now() - INTERVAL 30 DAY
GROUP BY user_id
ORDER BY event_count DESC
LIMIT 10;

-- Funnel d'√©v√©nements
WITH funnel AS (
    SELECT 
        user_id,
        session_id,
        arraySort(groupArray(event_type)) as event_sequence
    FROM events
    WHERE timestamp >= now() - INTERVAL 7 DAY
    GROUP BY user_id, session_id
)
SELECT 
    count() as total_sessions,
    countIf(has(event_sequence, 'page_view')) as page_views,
    countIf(has(event_sequence, 'click')) as clicks,
    countIf(has(event_sequence, 'purchase')) as purchases
FROM funnel;

-- M√©triques par host
SELECT 
    labels['host'] as host,
    metric_name,
    avg(metric_value) as avg_value,
    max(metric_value) as max_value,
    min(metric_value) as min_value
FROM metrics
WHERE timestamp >= now() - INTERVAL 1 HOUR
GROUP BY host, metric_name
ORDER BY host, metric_name;
```

### Agr√©gations avanc√©es

```sql
-- Agr√©gations par fen√™tres de temps
SELECT 
    toStartOfHour(timestamp) as hour,
    event_type,
    count() as event_count,
    uniq(user_id) as unique_users
FROM events
WHERE timestamp >= now() - INTERVAL 24 HOUR
GROUP BY hour, event_type
ORDER BY hour DESC, event_count DESC;

-- Top-K par groupe
SELECT 
    user_id,
    event_type,
    count() as event_count
FROM events
WHERE timestamp >= now() - INTERVAL 7 DAY
GROUP BY user_id, event_type
ORDER BY user_id, event_count DESC
LIMIT 3 BY user_id;

-- Corr√©lations
SELECT 
    corr(
        avgIf(metric_value, metric_name = 'cpu_usage'),
        avgIf(metric_value, metric_name = 'memory_usage')
    ) as cpu_memory_correlation
FROM metrics
WHERE timestamp >= now() - INTERVAL 1 HOUR
GROUP BY labels['host'];
```

### Vues mat√©rialis√©es

```sql
-- Vue mat√©rialis√©e pour les statistiques quotidiennes
CREATE MATERIALIZED VIEW daily_stats
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(date)
ORDER BY (date, event_type)
AS SELECT
    toDate(timestamp) as date,
    event_type,
    count() as event_count,
    uniq(user_id) as unique_users
FROM events
GROUP BY date, event_type;

-- Vue mat√©rialis√©e pour les m√©triques agr√©g√©es
CREATE MATERIALIZED VIEW hourly_metrics
ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(hour)
ORDER BY (hour, metric_name, host)
AS SELECT
    toStartOfHour(timestamp) as hour,
    metric_name,
    labels['host'] as host,
    avgState(metric_value) as avg_value,
    maxState(metric_value) as max_value,
    minState(metric_value) as min_value
FROM metrics
GROUP BY hour, metric_name, host;
```

### Optimisation des performances

```sql
-- Index pour optimiser les requ√™tes
ALTER TABLE events ADD INDEX idx_user_timestamp user_id TYPE minmax GRANULARITY 4;
ALTER TABLE events ADD INDEX idx_event_type event_type TYPE set(100) GRANULARITY 4;

-- Projections pour acc√©l√©rer les requ√™tes
ALTER TABLE events ADD PROJECTION user_events_proj (
    SELECT user_id, event_type, timestamp
    ORDER BY user_id, timestamp
);

-- Compression optimis√©e
ALTER TABLE events MODIFY SETTING compression_codec = 'ZSTD(3)';

-- Partitionnement intelligent
ALTER TABLE events DROP PARTITION '202401';
ALTER TABLE events DROP PARTITION '202402';
```

### Monitoring et m√©triques

```sql
-- V√©rifier l'utilisation du stockage
SELECT 
    table,
    formatReadableSize(sum(bytes)) as size,
    formatReadableSize(sum(primary_key_bytes_in_memory)) as primary_key_size,
    count() as parts
FROM system.parts
WHERE active
GROUP BY table
ORDER BY sum(bytes) DESC;

-- V√©rifier les requ√™tes lentes
SELECT 
    query,
    count() as executions,
    avg(query_duration_ms) as avg_duration_ms,
    max(query_duration_ms) as max_duration_ms
FROM system.query_log
WHERE type = 'QueryFinish'
    AND query_duration_ms > 1000
    AND event_time >= now() - INTERVAL 1 HOUR
GROUP BY query
ORDER BY avg_duration_ms DESC
LIMIT 10;

-- V√©rifier l'utilisation m√©moire
SELECT 
    metric,
    value,
    description
FROM system.metrics
WHERE metric IN ('MemoryUsage', 'MemoryTracking', 'MemoryTrackingInBackgroundProcessingPool')
ORDER BY value DESC;
```

---

## Prochaines √©tapes

1. **[D√©marrage rapide](quick-start.md)** : D√©ployez ClickHouse en 5 minutes
2. **[R√©f√©rence API](api-reference.md)** : Tous les param√®tres disponibles
3. **[Tutoriels](tutorials/)** : Guides avanc√©s
4. **[D√©pannage](troubleshooting.md)** : Solutions aux probl√®mes courants
5. **[Optimisation](optimization/)** : Techniques d'optimisation
6. **[Migration](migration/)** : Migration depuis d'autres bases 